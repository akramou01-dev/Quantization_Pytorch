{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3782a00c",
   "metadata": {},
   "source": [
    "# Quantization with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60bd12c",
   "metadata": {},
   "source": [
    "Quantization is doing computation and memory access with lower data precision , usualy with int8 instead of the floating point, this will results in many performance gains like reduction in memory bandwith, reduction in model size and faster inference due to the fast computation with int8 \n",
    "\n",
    "Quantization also comme with some additional costs like that the quantized models has less accuracy then the floating models, so we always try to minimize the gap between the full floating accuracy and the quantized accuracy\n",
    "\n",
    "\n",
    "3 types of quantization : \n",
    "1. **Dynamic Quantization**: the idea is to convert weights and activations into int8 just before doing the computation, and this will result in faster computation (matrix multiplication and convolutions on int8 more faster than floating), note that the results are saved into the memory in float format, we need to multiply to a factor scale ,that is determined in diffrent way from on to another quantization approaches, for converting from floating point to integers\n",
    "    \n",
    "    We use the torch.quantization.quantize_dynamic method that take multiple arguments : \n",
    "        the model to be quantized \n",
    "        the layer to be quantized\n",
    "        and type of the weights that will be quantized into \n",
    "        \n",
    "        \n",
    "Next , we are going to use the dynamic quantization with a simple LSTM model for demo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0b5bbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn \n",
    "import os \n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6894b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoModel(nn.Module): \n",
    "    def __init__(self,input_dim,out_dim):\n",
    "        super(DemoModel,self).__init__()\n",
    "        self.lay1 = nn.LSTM(input_dim,out_dim)\n",
    "        self.lay2= nn.LSTM(out_dim , 32)\n",
    "        self.lay3 = nn.Linear(32,10)\n",
    "    def forward(self,x):\n",
    "        out,hidden = self.lay1(x)\n",
    "        print(hidden)\n",
    "        out = nn.functional.relu(out)\n",
    "        out,_ = self.lay2(out)\n",
    "        out = nn.functional.relu(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return self.lay3(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3578c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim,sequence_len = 64,20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a5f8f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DemoModel(model_dim,sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "87564156",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = th.randn(sequence_len,1 , model_dim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "02d71738",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = (th.randn(1,1,model_dim),th.randn(1,1,model_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6db935aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = DemoModel(model_dim, model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9838b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_lstm = th.quantization.quantize_dynamic(lstm,{nn.LSTM,nn.Linear},dtype=th.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "324ee8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoModel(\n",
       "  (lay1): DynamicQuantizedLSTM(64, 64)\n",
       "  (lay2): DynamicQuantizedLSTM(64, 32)\n",
       "  (lay3): DynamicQuantizedLinear(in_features=32, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff391752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoModel(\n",
       "  (lay1): LSTM(64, 64)\n",
       "  (lay2): LSTM(64, 32)\n",
       "  (lay3): Linear(in_features=32, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "29b8df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    th.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "adc88165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Floating LSTM  \t Size (KB): 187271\n",
      "model:  Quantized LSTM  \t Size (KB): 38693071\n"
     ]
    }
   ],
   "source": [
    "model_size = print_size_of_model(lstm,\"Floating LSTM\")\n",
    "quantized_model_size = print_size_of_model(quantized_model,'Quantized LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d62b8311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoModel(\n",
       "  (lay1): DynamicQuantizedLSTM(64, 64)\n",
       "  (lay2): DynamicQuantizedLSTM(64, 32)\n",
       "  (lay3): DynamicQuantizedLinear(in_features=32, out_features=10, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a99b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
